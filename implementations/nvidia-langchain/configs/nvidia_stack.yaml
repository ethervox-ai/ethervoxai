# NVIDIA Stack Configuration for EthervoxAI
# Production-ready configuration for NVIDIA AI services

nvidia:
  # CUDA Configuration
  cuda:
    version: "12.1"
    devices: ["0", "1", "2", "3"]  # GPU device IDs to use
    memory_fraction: 0.95          # Fraction of GPU memory to allocate
    allow_growth: true             # Allow GPU memory to grow dynamically
    
  # TensorRT Optimization Settings
  tensorrt:
    enabled: true
    precision: "fp16"              # fp32, fp16, int8
    max_batch_size: 32
    max_workspace_size: "4GB"
    optimization_level: 5          # 0-5, higher = more optimization
    strict_type_constraints: false
    engine_cache_dir: "/app/cache/tensorrt"
    
  # Triton Inference Server Configuration
  triton:
    server_url: "triton-server:8001"
    model_repository: "/models"
    grpc_port: 8001
    http_port: 8000
    metrics_port: 8002
    instance_group_count: 2
    max_batch_size: 32
    preferred_batch_size: [8, 16]
    max_queue_delay_microseconds: 10000
    
  # NVIDIA NeMo Configuration
  nemo:
    framework: "pytorch"
    checkpoint_dir: "/app/models/nemo"
    cache_dir: "/app/cache/nemo"
    tensor_model_parallel_size: 2
    pipeline_model_parallel_size: 1
    sequence_parallel: false
    
  # NVIDIA Riva Configuration
  riva:
    server_url: "localhost:50051"
    language_models: 
      - "en-US"
      - "es-ES" 
      - "fr-FR"
      - "de-DE"
    asr_model: "citrinet_1024_gamma_0_25"
    tts_model: "fastpitch_hifigan"
    nlp_model: "bert_base_uncased"

# Model Configuration
models:
  # Large Language Models
  llm:
    primary:
      name: "llama2-70b-chat"
      type: "causal_lm"
      precision: "fp16"
      max_sequence_length: 4096
      model_path: "/app/models/llama2-70b-chat"
      
    fallback:
      name: "llama2-13b-chat"
      type: "causal_lm"
      precision: "fp16"
      max_sequence_length: 4096
      model_path: "/app/models/llama2-13b-chat"
      
  # Embedding Models
  embeddings:
    primary:
      name: "sentence-transformers/all-MiniLM-L6-v2"
      dimensions: 384
      max_sequence_length: 512
      
    large:
      name: "sentence-transformers/all-mpnet-base-v2"
      dimensions: 768
      max_sequence_length: 512

# Performance Optimization
performance:
  # Multi-GPU Strategy
  parallelism:
    strategy: "tensor_parallel"     # tensor_parallel, pipeline_parallel, data_parallel
    tensor_parallel_size: 4
    pipeline_parallel_size: 1
    data_parallel_size: 1
    
  # Memory Optimization
  memory:
    kv_cache_dtype: "fp16"
    attention_implementation: "flash_attention_2"
    gradient_checkpointing: false
    offload_activations: false
    cpu_offload: false
    
  # Inference Optimization
  inference:
    batch_size: 16
    max_batch_delay_ms: 50
    dynamic_batching: true
    sequence_batching: false
    padding_strategy: "longest"
    
  # Compilation Optimization
  compilation:
    torch_compile: true
    torch_compile_mode: "max-autotune"
    dynamo_backend: "inductor"

# Monitoring and Logging
monitoring:
  # Performance Metrics
  metrics:
    enabled: true
    port: 8002
    update_interval_seconds: 30
    
    # GPU Metrics
    gpu_metrics:
      - "utilization"
      - "memory_usage"
      - "temperature"
      - "power_consumption"
      
    # Model Metrics  
    model_metrics:
      - "inference_latency"
      - "throughput"
      - "queue_size"
      - "error_rate"
      
  # Logging Configuration
  logging:
    level: "INFO"                   # DEBUG, INFO, WARNING, ERROR
    format: "structured"            # structured, plain
    output: "stdout"               # stdout, file, both
    file_path: "/app/logs/ethervox.log"
    max_file_size: "100MB"
    backup_count: 5

# Security Configuration
security:
  # Authentication
  authentication:
    enabled: true
    method: "jwt"                  # jwt, api_key, oauth
    jwt_secret: "${JWT_SECRET}"
    token_expiry: 3600             # seconds
    
  # Authorization
  authorization:
    enabled: true
    roles:
      - name: "admin"
        permissions: ["read", "write", "admin"]
      - name: "user"
        permissions: ["read", "write"]
      - name: "readonly"
        permissions: ["read"]
        
  # Input Validation
  input_validation:
    max_input_length: 8192
    allowed_content_types: ["text/plain", "application/json"]
    rate_limiting:
      enabled: true
      requests_per_minute: 60
      burst_size: 10
      
  # Output Filtering
  output_filtering:
    enabled: true
    filter_pii: true               # Filter personally identifiable information
    filter_code: false             # Filter code snippets
    filter_urls: false             # Filter URLs

# Compliance Configuration
compliance:
  # GDPR Settings
  gdpr:
    enabled: true
    data_retention_days: 30
    anonymize_logs: true
    right_to_erasure: true
    
  # HIPAA Settings
  hipaa:
    enabled: false
    encryption_at_rest: true
    encryption_in_transit: true
    audit_logging: true
    
  # SOC2 Settings
  soc2:
    enabled: false
    access_logging: true
    change_management: true
    incident_response: true

# Deployment Configuration
deployment:
  # Scaling Configuration
  scaling:
    min_replicas: 1
    max_replicas: 10
    target_cpu_utilization: 70
    target_gpu_utilization: 80
    scale_up_cooldown: 300         # seconds
    scale_down_cooldown: 600       # seconds
    
  # Health Checks
  health_checks:
    liveness_probe:
      endpoint: "/health/live"
      initial_delay_seconds: 30
      period_seconds: 30
      timeout_seconds: 10
      failure_threshold: 3
      
    readiness_probe:
      endpoint: "/health/ready"
      initial_delay_seconds: 60
      period_seconds: 10
      timeout_seconds: 5
      failure_threshold: 3
      
  # Resource Limits
  resources:
    requests:
      cpu: "4"
      memory: "16Gi"
      nvidia.com/gpu: 2
      
    limits:
      cpu: "8" 
      memory: "32Gi"
      nvidia.com/gpu: 4

# Environment-specific Overrides
environments:
  development:
    nvidia:
      cuda:
        devices: ["0"]
      tensorrt:
        optimization_level: 3
    monitoring:
      logging:
        level: "DEBUG"
        
  staging:
    nvidia:
      cuda:
        devices: ["0", "1"]
    performance:
      parallelism:
        tensor_parallel_size: 2
        
  production:
    nvidia:
      cuda:
        devices: ["0", "1", "2", "3"]
        memory_fraction: 0.95
    performance:
      parallelism:
        tensor_parallel_size: 4
    security:
      authentication:
        enabled: true
    compliance:
      gdpr:
        enabled: true
