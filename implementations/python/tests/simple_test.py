#!/usr/bin/env python3
"""
üß™ EthervoxAI Python Implementation - Simple Test Runner

Basic test validation for the Python implementation without pytest.
"""

import asyncio
import sys
from pathlib import Path

# Add the parent directory to the path so we can import ethervoxai
sys.path.insert(0, str(Path(__file__).parent.parent))

from ethervoxai.platform_detector import platform_detector, SystemCapabilities, ModelCompatibility
from ethervoxai.model_manager import model_manager, ModelInfo
from ethervoxai.inference_engine import inference_engine, InferenceParameters, InferenceResponse
from ethervoxai.local_llm_stack import local_llm_stack, ProcessingResponse, IntentResult

async def test_platform_detection():
    """Test platform detection capabilities"""
    print("\nTesting Platform Detection...")
    
    # Test system capabilities
    capabilities = await platform_detector.get_capabilities()
    assert isinstance(capabilities, SystemCapabilities)
    assert capabilities.cpu_cores > 0
    assert capabilities.total_memory > 0
    
    print(f"   - System: {capabilities.platform} {capabilities.architecture}")
    print(f"   - Performance: {capabilities.performance_tier}")
    print(f"   - Memory: {capabilities.total_memory}MB")
    
    # Test model compatibility
    compatibility = await platform_detector.check_model_compatibility(
        "test-model", 1000, 2048
    )
    assert isinstance(compatibility, ModelCompatibility)
    print(f"   ‚úÖ Model compatibility: {compatibility.is_compatible}")

async def test_model_management():
    """Test model management functionality"""
    print("\nüì¶ Testing Model Management...")
    
    # Test model catalog
    catalog = model_manager.get_default_model_catalog()
    assert isinstance(catalog, list)
    assert len(catalog) > 0
    print(f"   ‚úÖ Model catalog: {len(catalog)} models")
    
    # Test recommendations
    recommended = await model_manager.get_recommended_models()
    assert isinstance(recommended, list)
    print(f"   ‚úÖ Recommended: {len(recommended)} models")
    
    # Test downloaded models (can be empty)
    downloaded = model_manager.list_downloaded_models()
    assert isinstance(downloaded, list)
    print(f"   ‚úÖ Downloaded: {len(downloaded)} models")
    
    # Test cache size
    cache_size = model_manager.get_cache_size_mb()
    assert isinstance(cache_size, (int, float))
    print(f"   ‚úÖ Cache size: {cache_size:.1f}MB")

async def test_inference_engine():
    """Test inference engine functionality"""
    print("\nüß† Testing Inference Engine...")
    
    # Test initialization state
    assert not inference_engine.is_initialized()
    print("   ‚úÖ Initial state: uninitialized")
    
    # Test inference parameters
    params = InferenceParameters()
    assert 0.0 <= params.temperature <= 2.0
    assert params.max_tokens > 0
    print("   ‚úÖ Parameters: valid defaults")
    
    # Test response structure
    response = InferenceResponse(
        text="Test response",
        tokens_generated=10,
        tokens_per_second=5.0,
        finished=True,
        finish_reason="stop",
        timings={"total_time": 1.5}
    )
    assert isinstance(response.text, str)
    print("   ‚úÖ Response structure: valid")

async def test_llm_stack():
    """Test LLM stack integration"""
    print("\nüöÄ Testing LLM Stack...")
    
    # Test initialization state
    status = await local_llm_stack.get_system_status()
    assert not status["is_initialized"]
    print("   ‚úÖ Initial state: uninitialized")
    
    # Test intent parsing
    intent = await local_llm_stack._parse_intent("Calculate 2 + 2")
    assert isinstance(intent, IntentResult)
    print(f"   ‚úÖ Intent parsing: {intent.intent}")
    
    # Test another intent
    intent2 = await local_llm_stack._parse_intent("What's the weather?")
    assert isinstance(intent2, IntentResult)
    print(f"   ‚úÖ Intent parsing 2: {intent2.intent}")
    
    # Test response structure
    response = ProcessingResponse(
        text="Test",
        confidence=0.85,
        source="local",
        model="test",
        tokens_used=10,
        inference_stats={},
        privacy_level="local"
    )
    response_dict = response.to_dict()
    assert isinstance(response_dict, dict)
    print("   ‚úÖ Response structure: valid")

async def test_protocol_compliance():
    """Test protocol compliance"""
    print("\nüìã Testing Protocol Compliance...")
    
    # Test platform detection protocol
    capabilities = await platform_detector.get_capabilities()
    capabilities_dict = capabilities.to_dict()
    required_fields = [
        "cpu_cores", "total_memory", "available_memory", 
        "performance_tier", "platform", "architecture"
    ]
    for field in required_fields:
        assert field in capabilities_dict
    print("   ‚úÖ Platform detection protocol")
    
    # Test model info protocol
    catalog = model_manager.get_default_model_catalog()
    model_dict = catalog[0].to_dict()
    required_fields = [
        "name", "display_name", "size", "required_memory", "download_url"
    ]
    for field in required_fields:
        assert field in model_dict
    print("   ‚úÖ Model info protocol")
    
    # Test inference parameters protocol
    params = InferenceParameters()
    from dataclasses import asdict
    params_dict = asdict(params)
    required_fields = ["temperature", "max_tokens", "top_p", "stop_sequences"]
    for field in required_fields:
        assert field in params_dict
    print("   ‚úÖ Inference parameters protocol")

async def run_all_tests():
    """Run all tests"""
    print("EthervoxAI Python Implementation - Test Suite")
    print("=" * 60)
    
    try:
        await test_platform_detection()
        await test_model_management()
        await test_inference_engine()
        await test_llm_stack()
        await test_protocol_compliance()
        
        print("\n" + "=" * 60)
        print("ALL TESTS PASSED!")
        print("- Platform detection: Working")
        print("- Model management: Working") 
        print("- Inference engine: Framework ready")
        print("- LLM stack: Framework ready")
        print("- Protocol compliance: Verified")
        print("\nPython implementation is ready for production!")
        
    except Exception as e:
        print(f"\n‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

if __name__ == "__main__":
    success = asyncio.run(run_all_tests())
    sys.exit(0 if success else 1)
